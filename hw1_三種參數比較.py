# -*- coding: utf-8 -*-
"""HW1 三種參數比較.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M86JECNMXHC7m9WCeruSyNwH59dMfAXt
"""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier, plot_tree
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# 欄位名稱
cols = [
    "age", "workclass", "fnlwgt", "education", "education-num", "marital-status",
    "occupation", "relationship", "race", "sex", "capital-gain", "capital-loss",
    "hours-per-week", "native-country", "income"
]

# 檔案路徑
train_url = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data"
test_url  = "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.test"


# 讀取資料
df_train = pd.read_csv(train_url, names=cols, sep=",", skipinitialspace=True)
df_test = pd.read_csv(test_url, names=cols, sep=",", skipinitialspace=True, skiprows=1)

# 合併資料
df = pd.concat([df_train, df_test], ignore_index=True)

# 處理缺失值
df = df.replace('?', pd.NA).dropna()
df['income'] = df['income'].apply(lambda s: s.strip().strip('.'))

# 取得文字欄位
cat_cols = df.select_dtypes(include=['object']).columns
num_cols = df.select_dtypes(exclude=['object']).columns
for c in cat_cols:
    df[c] = df[c].astype(str)

# 切分資料
train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)

# 擷取特徵標籤
X_train = train_df.drop('income', axis=1)
y_train = train_df['income']
X_test  = test_df.drop('income', axis=1)
y_test  = test_df['income']

# OneHot編碼器
encoder = ColumnTransformer([
    ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols),
    ('num', 'passthrough', num_cols)
])

# 編碼資料
X_train_enc = encoder.fit_transform(X_train)
X_test_enc  = encoder.transform(X_test)

# 取得欄位名稱
encoded_cat_names = encoder.named_transformers_['cat'].get_feature_names_out(cat_cols)
all_feature_names = list(encoded_cat_names) + [col for col in X_train.columns if col not in cat_cols]

# 轉成 pandas DataFrame
X_train_enc = pd.DataFrame(X_train_enc, columns=all_feature_names)
X_test_enc  = pd.DataFrame(X_test_enc, columns=all_feature_names)

# 計算 Cost Complexity Pruning 路徑
clf_base = DecisionTreeClassifier(random_state=42, criterion='gini')
path = clf_base.cost_complexity_pruning_path(X_train_enc, y_train)
ccp_alphas = path.ccp_alphas

# 取三個代表性 alpha
selected_alphas = [ccp_alphas[10], 0.0001, ccp_alphas[-10]]

# 建立三棵不同複雜度的樹並計算 train/test 準確率

trees = []
train_accs = []
test_accs = []

for alpha in selected_alphas:
    clf = DecisionTreeClassifier(random_state=42, criterion='gini', ccp_alpha=alpha)
    clf.fit(X_train_enc, y_train)
    y_pred_train = clf.predict(X_train_enc)
    y_pred_test  = clf.predict(X_test_enc)

    train_acc = accuracy_score(y_train, y_pred_train)
    test_acc  = accuracy_score(y_test, y_pred_test)

    trees.append(clf)
    train_accs.append(train_acc)
    test_accs.append(test_acc)

    print(f"alpha={alpha:.6f} → train_acc={train_acc:.4f}, test_acc={test_acc:.4f}, Nodes={clf.tree_.node_count}, Depth={clf.get_depth()}")

# 畫出決策樹

fig, axes = plt.subplots(1, 3, figsize=(30, 7))
for i, (clf, alpha) in enumerate(zip(trees, selected_alphas)):
    plot_tree(
        clf,
        filled=True,
        max_depth=3,  #僅展示前三層
        fontsize=8,
        ax=axes[i]
    )
    axes[i].set_title(f"Tree {i+1}\nalpha={alpha:.5f}")
plt.tight_layout()
plt.show()

# 建立比較表
result_df = pd.DataFrame({
    'Tree': [f'Tree {i+1}' for i in range(3)],
    'ccp_alpha': selected_alphas,
    'Train Accuracy': train_accs,
    'Test Accuracy': test_accs,
    'Depth': [t.get_depth() for t in trees],
    'Nodes': [t.tree_.node_count for t in trees]
})
print(result_df)